# ğŸ¤– AI SYSTEM V2 - IMPLEMENTATION COMPLETE

**Last Updated:** December 18, 2025  
**Status:** âœ… **Production Ready**

---

## ğŸ“‹ Tá»”NG QUAN (OVERVIEW)

Há»‡ thá»‘ng AI má»›i vá»›i kiáº¿n trÃºc **3-tier hybrid** Ä‘Ã£ Ä‘Æ°á»£c triá»ƒn khai hoÃ n chá»‰nh, káº¿t há»£p:
- **Tier 1:** Fast & Free (Groq, Google AI Studio)
- **Tier 2:** Powerful & Local (ProxyPal - Gemini 3 Pro, Qwen 3 Coder)
- **Tier 3:** Premium & Critical (ProxyPal Premium - GPT-5.2, GPT-5.1)

### TÃ­nh nÄƒng Ä‘Ã£ triá»ƒn khai (Implemented Features)

âœ… **AI Tutor Chatbot** - Real-time WebSocket chat vá»›i AI  
âœ… **Multi-Provider Support** - Tá»± Ä‘á»™ng chá»n provider phÃ¹ há»£p  
âœ… **Smart Model Selection** - AI Orchestrator tá»± Ä‘á»™ng phÃ¢n loáº¡i cÃ¢u há»i  
âœ… **Redis Caching** - Cache responses Ä‘á»ƒ tiáº¿t kiá»‡m chi phÃ­  
âœ… **Database Persistence** - LÆ°u lá»‹ch sá»­ chat vÃ o PostgreSQL  
âœ… **REST API** - HTTP endpoints Ä‘á»ƒ kiá»ƒm tra status vÃ  test  
âœ… **WebSocket Gateway** - Real-time streaming responses  
âœ… **Auto Fallback** - Tá»± Ä‘á»™ng chuyá»ƒn sang provider khÃ¡c náº¿u failed

---

## ğŸ—ï¸ Cáº¤U TRÃšC Dá»° ÃN (PROJECT STRUCTURE)

```
backend/src/modules/ai/
â”œâ”€â”€ providers/                    # AI Provider implementations
â”‚   â”œâ”€â”€ base.provider.ts         # Base interface cho táº¥t cáº£ providers
â”‚   â”œâ”€â”€ proxypal.provider.ts     # ProxyPal (Local: Gemini 3, Qwen 3)
â”‚   â”œâ”€â”€ groq.provider.ts         # Groq (Free: Llama 3 70B)
â”‚   â””â”€â”€ google-ai.provider.ts    # Google AI Studio (Free: Gemini Flash)
â”œâ”€â”€ orchestrator/                 # Model selection & routing
â”‚   â””â”€â”€ ai-orchestrator.ts       # PhÃ¢n loáº¡i cÃ¢u há»i vÃ  chá»n provider
â”œâ”€â”€ services/                     # Business logic
â”‚   â”œâ”€â”€ ai-tutor.service.ts      # AI Tutor chatbot service
â”‚   â””â”€â”€ ai-cache.service.ts      # Redis caching cho AI responses
â”œâ”€â”€ repositories/                 # Database operations
â”‚   â””â”€â”€ ai-chat.repository.ts    # CRUD cho chat history
â”œâ”€â”€ gateways/                     # WebSocket handlers
â”‚   â””â”€â”€ ai-chat.gateway.ts       # WebSocket gateway cho AI chat
â”œâ”€â”€ controllers/                  # HTTP controllers
â”‚   â””â”€â”€ ai-v2.controller.ts      # REST API endpoints
â”œâ”€â”€ routes/                       # API routes
â”‚   â””â”€â”€ ai-v2.routes.ts          # Route definitions
â””â”€â”€ index.ts                      # Module exports

backend/src/models/
â””â”€â”€ ai-chat-history.model.ts      # Sequelize model cho chat history

backend/migrations/
â””â”€â”€ 20251218-create-ai-chat-history.ts  # Database migration
```

---

## ğŸš€ HÆ¯á»šNG DáºªN TRIá»‚N KHAI (DEPLOYMENT GUIDE)

### BÆ°á»›c 1: Cáº¥u hÃ¬nh mÃ´i trÆ°á»ng (.env)

```env
# =========================================
# AI CONFIGURATION
# =========================================

# Google AI Studio (Free Tier) - Gemini Flash
GEMINI_API_KEY=your-gemini-api-key-here
GEMINI_MODEL=gemini-1.5-flash
GEMINI_TEMPERATURE=0.7
GEMINI_MAX_TOKENS=8192

# Groq API (Free Tier) - Llama 3
GROQ_API_KEY=your-groq-api-key-here
GROQ_MODEL=llama-3.3-70b-versatile

# ProxyPal (Local AI Gateway) - Optional
PROXYPAL_BASE_URL=http://localhost:8317
PROXYPAL_ENABLED=false

# AI Features Toggles
AI_TUTOR_ENABLED=true
AI_QUIZ_GENERATOR_ENABLED=true
AI_GRADER_ENABLED=false
AI_CONTENT_REPURPOSING_ENABLED=false

# Redis (Required for caching)
REDIS_URL=redis://localhost:6379
```

### BÆ°á»›c 2: Láº¥y API Keys

#### Google AI Studio (Free)
1. Truy cáº­p: https://aistudio.google.com/
2. ÄÄƒng nháº­p vá»›i Google account
3. Click "Get API Key"
4. Copy API key vÃ o `GEMINI_API_KEY`

#### Groq API (Free)
1. Truy cáº­p: https://console.groq.com/
2. ÄÄƒng kÃ½ tÃ i khoáº£n
3. Táº¡o API key
4. Copy API key vÃ o `GROQ_API_KEY`

#### ProxyPal (Optional - For Development)
1. Táº£i ProxyPal: https://proxypal.ai/download
2. CÃ i Ä‘áº·t vÃ  cháº¡y ProxyPal
3. ÄÄƒng nháº­p vá»›i Google/Alibaba accounts
4. Set `PROXYPAL_ENABLED=true`

### BÆ°á»›c 3: Cháº¡y Migration

```bash
# Cháº¡y migration Ä‘á»ƒ táº¡o báº£ng ai_chat_history
npm run migrate

# Hoáº·c reset database (náº¿u development)
npm run reset-db
```

### BÆ°á»›c 4: Khá»Ÿi Ä‘á»™ng Server

```bash
# Development mode
npm run dev

# Production mode
npm run build
npm start
```

### BÆ°á»›c 5: Kiá»ƒm tra Status

```bash
# Check AI service status
curl http://localhost:3000/api/v1/ai/status

# Expected response:
{
  "success": true,
  "data": {
    "available": true,
    "providers": [
      {
        "name": "Groq",
        "model": "llama-3.3-70b-versatile",
        "available": true
      },
      {
        "name": "Google AI",
        "model": "gemini-1.5-flash",
        "available": true
      }
    ],
    "cache": {
      "available": true,
      "stats": {
        "totalKeys": 0,
        "memoryUsed": "1.2M"
      }
    },
    "features": {
      "tutor": true,
      "quizGenerator": true,
      "grader": false,
      "contentRepurposing": false
    }
  }
}
```

---

## ğŸ“¡ API ENDPOINTS

### 1. Check AI Status
```http
GET /api/v1/ai/status
```

**Response:**
```json
{
  "success": true,
  "data": {
    "available": true,
    "providers": [...],
    "cache": {...},
    "features": {...}
  }
}
```

### 2. Test AI Chat (REST)
```http
POST /api/v1/ai/chat/test
Authorization: Bearer <token>
Content-Type: application/json

{
  "message": "Explain what is React?",
  "courseId": "uuid-optional",
  "lessonId": "uuid-optional"
}
```

**Response:**
```json
{
  "success": true,
  "data": {
    "response": "React is a JavaScript library...",
    "metadata": {
      "model": "llama-3.3-70b-versatile",
      "provider": "Groq",
      "tier": "tier1",
      "latency": 1250,
      "usage": {
        "promptTokens": 120,
        "completionTokens": 450,
        "totalTokens": 570
      }
    }
  }
}
```

### 3. Get Available Providers
```http
GET /api/v1/ai/providers
Authorization: Bearer <token>
```

### 4. Clear AI Cache (Admin)
```http
DELETE /api/v1/ai/cache
Authorization: Bearer <admin-token>
```

---

## ğŸ”Œ WEBSOCKET API

### Connection
```javascript
const socket = io('http://localhost:3000/ai/chat', {
  auth: {
    token: 'your-jwt-token'
  },
  query: {
    courseId: 'course-uuid',
    lessonId: 'lesson-uuid'
  }
});
```

### Events

#### Client â†’ Server

**Send Message:**
```javascript
socket.emit('message', {
  text: 'What is the difference between var and let?',
  courseId: 'optional-course-id',
  lessonId: 'optional-lesson-id'
});
```

**Get History:**
```javascript
socket.emit('get_history');
```

**Clear History:**
```javascript
socket.emit('clear_history');
```

#### Server â†’ Client

**Connected:**
```javascript
socket.on('connected', (data) => {
  console.log('Connected to AI Tutor:', data);
});
```

**Status Updates:**
```javascript
socket.on('status', (data) => {
  // data.state: 'typing' | 'idle'
  console.log('AI status:', data.state);
});
```

**Response Chunk (Streaming):**
```javascript
socket.on('response_chunk', (data) => {
  console.log('Chunk:', data.chunk);
  // Append to UI
});
```

**Complete Response:**
```javascript
socket.on('message_response', (data) => {
  console.log('Full response:', data.text);
  console.log('Metadata:', data.metadata);
});
```

**History:**
```javascript
socket.on('history', (data) => {
  console.log('Conversation history:', data.messages);
});
```

**Error:**
```javascript
socket.on('error', (data) => {
  console.error('Error:', data.message);
});
```

---

## ğŸ¯ SMART MODEL SELECTION

AI Orchestrator tá»± Ä‘á»™ng phÃ¢n loáº¡i cÃ¢u há»i vÃ  chá»n provider phÃ¹ há»£p:

### Question Types

| Type | Complexity | Provider | Reason |
|------|-----------|----------|---------|
| **Code** | High | ProxyPal (Qwen Coder) | Code-specialized model |
| **Complex** | High | ProxyPal (Gemini Pro) | Large context window |
| **Math** | Medium | Google AI (Gemini Flash) | Fast + accurate |
| **Simple** | Low | Groq (Llama 3) | Fastest response |

### Fallback Chain

```
Primary Provider Failed
    â†“
Groq (Tier 1 - Fast)
    â†“ (if failed)
Google AI (Tier 1 - Reliable)
    â†“ (if failed)
Error: All providers unavailable
```

---

## ğŸ’¾ DATABASE SCHEMA

### Table: `ai_chat_history`

```sql
CREATE TABLE ai_chat_history (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
  course_id UUID REFERENCES courses(id) ON DELETE SET NULL,
  lesson_id UUID REFERENCES lessons(id) ON DELETE SET NULL,
  role VARCHAR(20) NOT NULL CHECK (role IN ('user', 'assistant', 'system')),
  message TEXT NOT NULL,
  model VARCHAR(100),
  provider VARCHAR(50),
  latency INTEGER COMMENT 'Response latency in milliseconds',
  tokens_used INTEGER,
  created_at TIMESTAMP NOT NULL DEFAULT NOW(),
  updated_at TIMESTAMP NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_ai_chat_history_user_created ON ai_chat_history(user_id, created_at);
CREATE INDEX idx_ai_chat_history_course ON ai_chat_history(course_id);
CREATE INDEX idx_ai_chat_history_lesson ON ai_chat_history(lesson_id);
```

---

## ğŸ”’ REDIS CACHING

### Cache Strategy

- **Key Format:** `ai:cache:<sha256(prompt+context)>`
- **TTL:** 1 hour (3600 seconds)
- **Database:** Redis DB 3

### Cache Hit Rate

```bash
# Check cache stats
curl http://localhost:3000/api/v1/ai/status | jq '.data.cache'
```

---

## ğŸ“Š MONITORING & LOGGING

### Logs

All AI operations are logged vá»›i format:
```
[AIOrchestrator] Question classified as: code (high)
[AIOrchestrator] Selected: Qwen Coder (Code-specialized model)
[AITutorService] Processing question: type=code, complexity=high
[AITutorService] Response generated: provider=ProxyPal, latency=3250ms
[AIChatGateway] User <uuid> connected to AI chat
```

### Metrics

- Total messages processed
- Average latency per provider
- Token usage
- Cache hit rate
- Provider availability

---

## ğŸ”§ TROUBLESHOOTING

### ProxyPal khÃ´ng káº¿t ná»‘i Ä‘Æ°á»£c

```bash
# Check if ProxyPal is running
curl http://localhost:8317/v1/models

# If not running:
# 1. Start ProxyPal application
# 2. Login to Google/Alibaba accounts
# 3. Verify ports 8317 is not blocked
```

### Groq rate limit exceeded

```
Error: Groq rate limit exceeded. Please try again later.
```

**Solution:** System sáº½ tá»± Ä‘á»™ng fallback sang Google AI. Äá»£i 1 phÃºt vÃ  thá»­ láº¡i.

### Redis connection failed

```
[AICacheService] Redis not available, caching disabled
```

**Solution:** Caching khÃ´ng báº¯t buá»™c. Service váº«n hoáº¡t Ä‘á»™ng nhÆ°ng khÃ´ng cache responses.

### All providers unavailable

```
Error: No AI provider available. Please check your configuration.
```

**Check:**
1. `GEMINI_API_KEY` cÃ³ Ä‘Ãºng khÃ´ng?
2. `GROQ_API_KEY` cÃ³ Ä‘Ãºng khÃ´ng?
3. Internet connection cÃ³ á»•n Ä‘á»‹nh khÃ´ng?
4. API keys cÃ³ háº¿t quota khÃ´ng?

---

## ğŸ“ˆ ROADMAP

### Phase 1: âœ… COMPLETED (December 18, 2025)
- [x] AI Tutor Chatbot
- [x] Multi-provider support
- [x] Smart model selection
- [x] WebSocket real-time chat
- [x] Redis caching
- [x] Database persistence

### Phase 2: ğŸš§ IN PROGRESS
- [ ] Quiz Generator
- [ ] AI Grader (Code + Essay)
- [ ] Debate Workflow (Multi-agent)

### Phase 3: ğŸ“‹ PLANNED
- [ ] Content Repurposing (Video â†’ Notes)
- [ ] Adaptive Learning Paths
- [ ] Voice Chat (Whisper STT)

---

## ğŸ‘¥ CREDITS

**Architect & Implementation:** GitHub Copilot  
**Documentation:** [00_INDEX.md](../../../docs/AI/00_INDEX.md)  
**Date:** December 18, 2025

---

## ğŸ“ SUPPORT

For issues or questions:
1. Check [troubleshooting section](#-troubleshooting)
2. Review logs: `backend/logs/`
3. Check AI documentation: `docs/AI/`

---

**ğŸ‰ AI System V2 is now live and ready for production!**
