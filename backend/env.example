# =========================================
# ENVIRONMENT CONFIGURATION
# =========================================
NODE_ENV=development
PORT=3000
LOG_LEVEL=info

# =========================================
# DATABASE CONFIGURATION
# =========================================
# PostgreSQL Database URL
# Format: postgresql://username:password@host:port/database
# For Docker: postgresql://lms_user:123456@postgres:5432/lms_db
# For Local: postgresql://lms_user:123456@localhost:5432/lms_db
DATABASE_URL=postgresql://lms_user:123456@localhost:5432/lms_db

# =========================================
# REDIS CONFIGURATION
# =========================================
# Redis URL for caching and session storage
# For Docker: redis://redis:6379
# For Local: redis://localhost:6379
REDIS_URL=redis://localhost:6379

# =========================================
# JWT CONFIGURATION
# =========================================
# JWT Secret Key (CHANGE IN PRODUCTION!)
JWT_SECRET=nguyensyphuctrankimhuongnguyenthanhloc
JWT_EXPIRES_IN=24h
JWT_REFRESH_EXPIRES_IN=7d
JWT_ISSUER=lms-backend
JWT_AUDIENCE=lms-frontend

# =========================================
# FRONTEND CONFIGURATION
# =========================================
# Frontend URL for CORS and redirects
FRONTEND_URL=http://localhost:3000

# Centralized CORS configuration (used by config/cors.config.ts)
# Multiple origins separated by comma
CORS_ALLOWED_ORIGINS=http://localhost:3000,http://localhost:5173,http://localhost:3001
# Allowed methods
CORS_ALLOWED_METHODS=GET,POST,PUT,PATCH,DELETE,OPTIONS
# Allowed headers
CORS_ALLOWED_HEADERS=Content-Type,Authorization,X-Requested-With
# Whether to send credentials (cookies/authorization headers)
CORS_ALLOW_CREDENTIALS=true

# =========================================
# FILE UPLOAD CONFIGURATION
# =========================================
# Maximum file size in bytes (10MB)
MAX_FILE_SIZE=10485760
# Upload directory path
UPLOAD_PATH=./uploads
PUBLIC_URL=http://localhost:3000
STORAGE_TYPE=local # local | google_cloud | aws_s3 | azure_blob

# =========================================
# EMAIL CONFIGURATION
# =========================================
# SMTP Configuration for sending emails
MAIL_HOST=smtp.gmail.com
MAIL_PORT=587
MAIL_SECURE=false
MAIL_USER=
MAIL_PASS=
MAIL_FROM=LMS System <noreply@lms.com>

# Alternative email configuration names
EMAIL_HOST=
EMAIL_PORT=587
EMAIL_USER=
EMAIL_PASS=
EMAIL_FROM=

# =========================================
# EXTERNAL SERVICES CONFIGURATION
# =========================================
# Cloud Storage Configuration (future)
# Google Cloud Storage
GCP_PROJECT_ID=
GCS_BUCKET=
GOOGLE_APPLICATION_CREDENTIALS=./config/gcs-service-account.json
GCS_PUBLIC_URL=

# Cloudinary (images)
CLOUDINARY_CLOUD_NAME=
CLOUDINARY_API_KEY=
CLOUDINARY_API_SECRET=
CLOUDINARY_UPLOAD_PRESET=

# =========================================
# RATE LIMITING CONFIGURATION
# =========================================
# Rate limiting settings
RATE_LIMIT_WINDOW_MS=900000
RATE_LIMIT_MAX_REQUESTS=100

# =========================================
# CACHE CONFIGURATION
# =========================================
# Cache TTL settings (in seconds)
CACHE_TTL_SHORT=300
CACHE_TTL_MEDIUM=1800
CACHE_TTL_LONG=3600

# =========================================
# SECURITY CONFIGURATION
# =========================================
# Password security settings
BCRYPT_ROUNDS=12
PASSWORD_MIN_LENGTH=8
PASSWORD_MAX_LENGTH=128

# Session settings
SESSION_SECRET=your-session-secret-key-change-in-production
SESSION_MAX_AGE=86400000

# =========================================
# MONITORING CONFIGURATION
# =========================================
# Health check settings
HEALTH_CHECK_TIMEOUT=5000
HEALTH_CHECK_INTERVAL=30000

# Metrics settings
METRICS_ENABLED=true
METRICS_PORT=9090

# OpenTelemetry Configuration
# Set to true only in production or when you have OTLP collector running
ENABLE_OTLP=false
OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://localhost:4318/v1/traces

# =========================================
# API VERSIONING CONFIGURATION
# =========================================
# Default API version
DEFAULT_API_VERSION=v1.3.0
# Supported API versions (comma-separated)
SUPPORTED_API_VERSIONS=v1.0.0,v1.1.0,v1.2.0,v2.0.0

# =========================================
# DEVELOPMENT CONFIGURATION
# =========================================
# Development-specific settings
DEBUG=false
VERBOSE_LOGGING=false
HOT_RELOAD=true

# =========================================
# GOOGLE DRIVE CONFIGURATION (OAuth 2.0)
# =========================================
# Thông tin OAuth 2.0 Client (lấy từ Google Cloud Console)
GOOGLE_CLIENT_ID=your-google-oauth-client-id
GOOGLE_CLIENT_SECRET=your-google-oauth-client-secret
GOOGLE_REFRESH_TOKEN=your-google-oauth-refresh-token
GOOGLE_REDIRECT_URI=https://developers.google.com/oauthplayground

# ID thư mục gốc chứa tài liệu khóa học (Course Resources)
# Ví dụ: LMS_DATA/Course_Resources
GDRIVE_RESOURCES_FOLDER_ID=your-drive-folder-id-for-course-resources

# ID thư mục gốc chứa raw livestream recordings
GDRIVE_LIVE_RAW_FOLDER_ID=your-drive-folder-id-for-live-raw

# ID thư mục gốc chứa backup database
# Có thể dùng GOOGLE_DRIVE_FOLDER_ID cũ như một fallback nếu muốn
GDRIVE_BACKUP_FOLDER_ID=your-drive-folder-id-for-db-backups

# =========================================
# AI CONFIGURATION (3-Tier Architecture)
# =========================================
# Hệ thống AI sử dụng chiến lược 3 tầng để tối ưu chi phí và hiệu năng:
# - Tier 1 (Fast + Free): Groq, Google AI Studio - Cho real-time chat, bulk processing
# - Tier 2 (Powerful + Local): ProxyPal - Cho tasks phức tạp, large context
# - Tier 3 (Premium): MegaLLM - Chỉ dùng cho critical operations

# --------------------------------------------
# TIER 1: Fast + Free APIs
# --------------------------------------------

# Google AI Studio (Free Tier) - Multi-Model Strategy
# Lấy API key tại: https://aistudio.google.com/
# STRATEGY: Use 4 models sequentially to maximize daily quota
# Each model: 20 RPD → Total: 80 RPD (4 models)
# Context window: Up to 2M tokens (varies by model)
# Use case: Production fallback when Groq busy, AI Tutor, bulk processing
GEMINI_API_KEY=your-gemini-api-key-here
GEMINI_TEMPERATURE=0.7
GEMINI_MAX_TOKENS=8192

# Google AI Models - Sequential Fallback (20 RPD each)
# When one model exhausts daily quota (429 error), automatically fallback to next
GOOGLE_MODEL_3_FLASH=gemini-3-flash                         # 20 RPD - Best quality
GOOGLE_MODEL_2_5_FLASH=gemini-2.5-flash                     # 20 RPD - General purpose
GOOGLE_MODEL_2_5_FLASH_LITE=gemini-2.5-flash-lite          # 20 RPD - Lightweight, faster
GOOGLE_MODEL_ROBOTICS=gemini-robotics-er-1.5-preview       # 20 RPD - Complex reasoning

# Groq API (Free Tier) - Multiple Models Available
# Lấy API key tại: https://console.groq.com/
# Rate limit: Unlimited RPD (free tier), latency: 0.5-2s
# 
# Available Models và Use Cases:
# - llama-3.3-70b-versatile: General chat, reasoning, AI Tutor (PRIMARY)
# - llama-4-scout: Vision, multimodal tasks (khi cần xử lý ảnh)
# - qwen-3-32b: Math, logic, technical reasoning
# - whisper-large-v3: Speech to text transcription
# - gpt-oss-120b: Advanced reasoning (khi Llama 3.3 không đủ)
#
GROQ_API_KEY=your-groq-api-key-here

# Model mặc định cho general tasks
GROQ_MODEL_DEFAULT=llama-3.3-70b-versatile

# Models cho specific tasks (optional, sẽ fallback về default nếu không set)
GROQ_MODEL_REASONING=llama-3.3-70b-versatile
GROQ_MODEL_MATH=qwen-3-32b
GROQ_MODEL_VISION=llama-4-scout
GROQ_MODEL_SPEECH=whisper-large-v3

# General settings
GROQ_TEMPERATURE=0.7
GROQ_MAX_TOKENS=2048

# --------------------------------------------
# TIER 2: Powerful + Local (ProxyPal)
# --------------------------------------------

# ProxyPal - Local AI Gateway
# Download tại: https://proxypal.ai/download
# Cần chạy ProxyPal trên máy local trước khi enable
# Models available qua ProxyPal:
#   - gemini-3-pro-preview (2M context) - Cho content repurposing, quiz generation
#   - qwen3-coder-plus (32K context) - Cho code grading, technical review
#   - qwen3-coder-flash (128K context) - Cho code generation, fast review
#
# QUAN TRỌNG: Dùng 127.0.0.1 thay vì localhost để tránh lỗi IPv6 trong Node.js 17+
# Nếu chạy backend trong Docker, dùng: http://host.docker.internal:8317/v1
PROXYPAL_BASE_URL=http://127.0.0.1:8317/v1
PROXYPAL_API_KEY=proxypal-local
# Đặt thành true khi ProxyPal đang chạy trên máy
PROXYPAL_ENABLED=false
PROXYPAL_TIMEOUT=60000

# --------------------------------------------
# TIER 3: Premium (Optional)
# --------------------------------------------

# MegaLLM - Premium AI Provider
# Chỉ dùng cho critical operations:
#   - Grade appeals (học viên kháng nghị điểm)
#   - Final exam generation
#   - Multi-agent debate arbitration
# Models: Claude Sonnet 4.5 ($3/$15 per M tokens), Claude Opus 4.5 ($5/$25 per M tokens)
# Budget: $150 total credit
# API Documentation: https://megallm.com/docs (check their docs for exact base URL)
# Common base URLs: https://api.megallm.com/v1 or https://megallm.com/api/v1
MEGALM_API_KEY=
MEGALM_BASE_URL=https://api.megallm.com/v1

# --------------------------------------------
# AI Features Toggles
# --------------------------------------------
# Bật/tắt các tính năng AI (true/false)
AI_TUTOR_ENABLED=true
AI_QUIZ_GENERATOR_ENABLED=true
AI_GRADER_ENABLED=false
AI_CONTENT_REPURPOSING_ENABLED=false

# =========================================
# PRODUCTION CONFIGURATION
# =========================================
# Production-specific settings (uncomment for production)
# NODE_ENV=production
# LOG_LEVEL=warn
# DEBUG=false
# VERBOSE_LOGGING=false
# HOT_RELOAD=false