# ğŸš€ HÆ¯á»šNG DáºªN SETUP Há»† THá»NG AI Má»šI

**NgÃ y táº¡o:** 18/12/2025  
**Tráº¡ng thÃ¡i:** âœ… Sáºµn sÃ ng sá»­ dá»¥ng

---

## ğŸ“‹ Tá»”NG QUAN

Há»‡ thá»‘ng AI má»›i Ä‘Ã£ Ä‘Æ°á»£c tÃ­ch há»£p hoÃ n toÃ n vÃ o backend vá»›i kiáº¿n trÃºc 3-Tier:

- **Tier 1 (Fast + Free):** Groq Llama 3, Google Gemini Flash
- **Tier 2 (Powerful + Local):** ProxyPal (Gemini 3 Pro, Qwen Coder)
- **Tier 3 (Premium):** MegaLLM (Claude Sonnet/Opus)

### âœ… ÄÃ£ hoÃ n thÃ nh

- [x] Cáº¥u hÃ¬nh environment variables trong `env.example`
- [x] Sá»­a táº¥t cáº£ lá»—i TypeScript (type-check passed)
- [x] Sá»­a táº¥t cáº£ lá»—i lint (lint passed)
- [x] Import paths Ä‘Ã£ Ä‘Æ°á»£c sá»­a Ä‘Ãºng
- [x] AI Chat Gateway Ä‘Ã£ Ä‘Æ°á»£c tÃ­ch há»£p vÃ o server.ts
- [x] Routes v2 Ä‘Ã£ Ä‘Æ°á»£c mount trong API v1
- [x] Model pattern Ä‘Ã£ Ä‘Æ°á»£c Ä‘iá»u chá»‰nh theo chuáº©n project

---

## ğŸ”§ CÃC BÆ¯á»šC SETUP

### BÆ°á»›c 1: Cáº¥u hÃ¬nh API Keys

Má»Ÿ file `.env` cá»§a báº¡n vÃ  thÃªm cÃ¡c dÃ²ng sau:

```bash
# =========================================
# AI CONFIGURATION (3-Tier Architecture)
# =========================================

# TIER 1: Fast + Free APIs

# Google AI Studio (Free Tier)
# Láº¥y táº¡i: https://aistudio.google.com/
GEMINI_API_KEY=your-actual-api-key-here
GEMINI_MODEL=gemini-2.5-flash
GEMINI_TEMPERATURE=0.7
GEMINI_MAX_TOKENS=8192

# Groq API (Free Tier) - Primary cho AI Tutor
# Láº¥y táº¡i: https://console.groq.com/
GROQ_API_KEY=your-actual-api-key-here
GROQ_MODEL=llama-3.3-70b-versatile
GROQ_TEMPERATURE=0.7
GROQ_MAX_TOKENS=2048

# TIER 2: ProxyPal (Optional - chá»‰ dÃ¹ng cho dev/testing)
PROXYPAL_BASE_URL=http://localhost:8317
PROXYPAL_ENABLED=false
PROXYPAL_TIMEOUT=60000

# TIER 3: MegaLLM (Optional - chá»‰ dÃ¹ng cho critical operations)
MEGALM_API_KEY=
MEGALM_BASE_URL=

# AI Features Toggles
AI_TUTOR_ENABLED=true
AI_QUIZ_GENERATOR_ENABLED=true
AI_GRADER_ENABLED=false
AI_CONTENT_REPURPOSING_ENABLED=false
```

### BÆ°á»›c 2: Cháº¡y Database Migration

```bash
npm run migrate
```

Migration sáº½ táº¡o báº£ng `ai_chat_history` vá»›i cáº¥u trÃºc:
- id, user_id, course_id, lesson_id
- role (user/assistant/system)
- message, model, provider
- latency, tokens_used
- timestamps

### BÆ°á»›c 3: Khá»Ÿi Ä‘á»™ng Server

```bash
npm run dev
```

Kiá»ƒm tra logs Ä‘á»ƒ Ä‘áº£m báº£o:
```
âœ… AI Service: Available (Gemini API connected)
   Model: gemini-2.5-flash
[AIOrchestrator] Initialized 5 providers
[AIOrchestrator] Fallback chain: groq -> google
[AIChatGateway] WebSocket namespace /ai/chat initialized
```

---

## ğŸ§ª KIá»‚M TRA Há»† THá»NG

### Test 1: Kiá»ƒm tra Status

```bash
curl http://localhost:3000/api/v1/ai/status
```

**Káº¿t quáº£ mong Ä‘á»£i:**
```json
{
  "success": true,
  "data": {
    "available": true,
    "providers": {
      "groq": true,
      "google": true,
      "proxypal-gemini": false,
      "proxypal-qwen-plus": false,
      "proxypal-qwen-flash": false
    },
    "activeProviders": 2
  }
}
```

### Test 2: Test Chat (REST API)

```bash
curl -X POST http://localhost:3000/api/v1/ai/chat/test \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_JWT_TOKEN" \
  -d '{
    "message": "Giáº£i thÃ­ch cho tÃ´i vá» React hooks"
  }'
```

### Test 3: Test WebSocket Chat

Sá»­ dá»¥ng Socket.IO client:

```javascript
import io from 'socket.io-client';

const socket = io('http://localhost:3000/ai/chat', {
  auth: {
    token: 'YOUR_JWT_TOKEN'
  },
  query: {
    courseId: 'optional-course-uuid',
    lessonId: 'optional-lesson-uuid'
  }
});

socket.on('connected', (data) => {
  console.log('âœ… Káº¿t ná»‘i thÃ nh cÃ´ng:', data);
});

socket.emit('message', {
  text: 'Giáº£i thÃ­ch cho tÃ´i vá» React hooks'
});

socket.on('response_chunk', (chunk) => {
  console.log('ğŸ“ Chunk:', chunk.text);
});

socket.on('message_response', (response) => {
  console.log('âœ… HoÃ n thÃ nh:', response);
});
```

---

## ğŸ—ï¸ Cáº¤U TRÃšC Há»† THá»NG Má»šI

### Providers (Tier 1 + 2)

```
backend/src/modules/ai/providers/
â”œâ”€â”€ base.provider.ts          # Base interface
â”œâ”€â”€ groq.provider.ts          # Groq Llama 3 (Primary)
â”œâ”€â”€ google-ai.provider.ts     # Google Gemini Flash (Fallback)
â””â”€â”€ proxypal.provider.ts      # ProxyPal gateway (Dev only)
```

### Orchestrator (Smart Routing)

```
backend/src/modules/ai/orchestrator/
â””â”€â”€ ai-orchestrator.ts        # Chá»n model dá»±a trÃªn:
                              # - Question complexity
                              # - Token count
                              # - Real-time requirement
```

### Services

```
backend/src/modules/ai/services/
â”œâ”€â”€ ai-tutor.service.ts       # AI Tutor business logic
â””â”€â”€ ai-cache.service.ts       # Redis caching
```

### Gateways

```
backend/src/modules/ai/gateways/
â””â”€â”€ ai-chat.gateway.ts        # WebSocket namespace: /ai/chat
```

### Controllers & Routes

```
backend/src/modules/ai/controllers/
â””â”€â”€ ai-v2.controller.ts       # REST API endpoints

backend/src/modules/ai/routes/
â””â”€â”€ ai-v2.routes.ts           # Mounted at /api/v1/ai
```

---

## ğŸ”€ LUá»’NG HOáº T Äá»˜NG

### AI Tutor Chat Flow

```
1. User connects WebSocket â†’ /ai/chat
   â†“
2. Authenticate with JWT token
   â†“
3. User sends message
   â†“
4. AI Orchestrator classifies question
   â†“
5. Select provider:
   - Simple question â†’ Groq (0.5-1.5s)
   - Complex question â†’ Google Flash (1-3s)
   - Code question â†’ ProxyPal Qwen (dev only)
   â†“
6. Provider generates response (with streaming)
   â†“
7. Save to ai_chat_history
   â†“
8. Cache response (1 hour TTL)
   â†“
9. Stream chunks to client
   â†“
10. Send final response
```

### Fallback Chain

```
Primary: Groq Llama 3
   â†“ (if error/rate limit)
Fallback 1: Google Gemini Flash
   â†“ (if error)
Fallback 2: Return error message
```

---

## ğŸ“Š MONITORING

### Log Patterns

```bash
# Theo dÃµi AI requests
docker logs lms-backend-dev -f | grep "\[AI"

# Theo dÃµi provider selection
docker logs lms-backend-dev -f | grep "Orchestrator"

# Theo dÃµi WebSocket connections
docker logs lms-backend-dev -f | grep "AIChatGateway"
```

### Key Metrics

- **Latency:** Groq < 2s, Google < 3s
- **Cache hit rate:** > 30% (tÃ¹y use case)
- **Provider availability:** > 99%
- **WebSocket connections:** Active sessions

---

## ğŸ› TROUBLESHOOTING

### Issue 1: "Groq provider not available"

**NguyÃªn nhÃ¢n:** GROQ_API_KEY khÃ´ng cÃ³ hoáº·c sai

**Giáº£i phÃ¡p:**
1. Kiá»ƒm tra `.env`: `GROQ_API_KEY=your-key`
2. Láº¥y key má»›i táº¡i: https://console.groq.com/
3. Restart server

### Issue 2: "Google AI provider not available"

**NguyÃªn nhÃ¢n:** GEMINI_API_KEY khÃ´ng cÃ³ hoáº·c sai

**Giáº£i phÃ¡p:**
1. Kiá»ƒm tra `.env`: `GEMINI_API_KEY=your-key`
2. Láº¥y key má»›i táº¡i: https://aistudio.google.com/
3. Restart server

### Issue 3: WebSocket connection rejected

**NguyÃªn nhÃ¢n:** JWT token khÃ´ng há»£p lá»‡

**Giáº£i phÃ¡p:**
1. Äáº£m báº£o token Ä‘Æ°á»£c gá»­i trong `auth` hoáº·c `query`
2. Token pháº£i valid vÃ  chÆ°a expired
3. Kiá»ƒm tra JWT_SECRET trong `.env`

### Issue 4: "ai_chat_history table not found"

**NguyÃªn nhÃ¢n:** Migration chÆ°a cháº¡y

**Giáº£i phÃ¡p:**
```bash
npm run migrate
```

---

## ğŸ¯ NEXT STEPS

Sau khi setup xong, báº¡n cÃ³ thá»ƒ:

1. **Test Frontend Integration**
   - TÃ­ch há»£p Socket.IO client trong React
   - Hiá»ƒn thá»‹ streaming responses
   - Implement chat history

2. **Enable Additional Features**
   - Báº­t AI_QUIZ_GENERATOR_ENABLED=true
   - Báº­t AI_GRADER_ENABLED=true (sau khi test)

3. **Setup ProxyPal (Optional)**
   - Download ProxyPal tá»« https://proxypal.ai/
   - Cáº¥u hÃ¬nh models: Gemini 3 Pro, Qwen Coder
   - Set PROXYPAL_ENABLED=true

4. **Monitor & Optimize**
   - Theo dÃµi latency
   - Äiá»u chá»‰nh cache TTL
   - Fine-tune model selection logic

---

## ğŸ“š TÃ€I LIá»†U THAM KHáº¢O

- [AI_IMPLEMENTATION_COMPLETE.md](src/modules/ai/AI_IMPLEMENTATION_COMPLETE.md) - TÃ i liá»‡u chi tiáº¿t
- [QUICK_START.md](src/modules/ai/QUICK_START.md) - HÆ°á»›ng dáº«n nhanh
- [docs/AI/05_AI_TUTOR.md](../docs/AI/05_AI_TUTOR.md) - TÃ i liá»‡u AI Tutor

---

**ğŸ‰ ChÃºc má»«ng! Há»‡ thá»‘ng AI Ä‘Ã£ sáºµn sÃ ng sá»­ dá»¥ng.**

Náº¿u gáº·p váº¥n Ä‘á», hÃ£y kiá»ƒm tra logs vÃ  tham kháº£o pháº§n Troubleshooting.
